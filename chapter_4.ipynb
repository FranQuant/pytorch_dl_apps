{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa40c929-d4d1-4c3c-87b4-dbc6b1c9c104",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "# Part I — Foundations of Machine Learning\n",
    "## Chapter 4 — The Limits of Classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c488cd-83ed-48f7-89d9-ab6a54acb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fab30d-d15b-4f3f-9883-a3a78c93a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1. Load ADR Dataset and Build Features (Chronologically Indexed)\n",
    "# ==============================================================\n",
    "\n",
    "# Load ADR data\n",
    "df = pd.read_csv(\"adr_prices_and_vol.csv\", parse_dates=[\"Date\"])\n",
    "ticker = \"CIB\"\n",
    "\n",
    "# Select relevant columns and drop missing data\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\", f\"{ticker}_Volume\"]].dropna().copy()\n",
    "df_t.set_index(\"Date\", inplace=True)  #  Ensure time-based indexing\n",
    "\n",
    "# Compute daily returns and volume changes\n",
    "df_t[\"Return_1d\"] = df_t[f\"{ticker}_Price\"].pct_change()\n",
    "df_t[\"VolChange\"] = df_t[f\"{ticker}_Volume\"].pct_change()\n",
    "\n",
    "# Compute 5-day returns and rolling volatility\n",
    "df_t[\"Return_5d\"] = df_t[f\"{ticker}_Price\"].pct_change(5)\n",
    "df_t[\"Volatility_5d\"] = df_t[\"Return_1d\"].rolling(5).std()\n",
    "\n",
    "# Drop missing values after feature creation\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# Binary target: 1 = Up day, 0 = Down day\n",
    "df_t[\"Target\"] = (df_t[\"Return_1d\"].shift(-1) > 0).astype(int)\n",
    "\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# Preview last few rows\n",
    "df_t.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5255f-adb2-4485-9313-acd1b5d26ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee400b-a937-4ae1-b435-0c32d3c2e51e",
   "metadata": {},
   "source": [
    "# Distance concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce59525-0dff-492a-a3df-bf4a6865752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 2. Distance Concentration (CIB ADR + Synthetic Expansion)\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Base Features from Real ADR Data\n",
    "# --------------------------------------------------------------\n",
    "features_core = [\"Return_1d\", \"VolChange\", \"Return_5d\", \"Volatility_5d\"]\n",
    "df_features = df_t[features_core].copy()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Synthetic Expansion — Lagged and Noise Features\n",
    "#    (simulate high-dimensional noisy financial feature space)\n",
    "# --------------------------------------------------------------\n",
    "# Add lagged returns (simulate memory effects)\n",
    "for lag in [2, 3, 4, 5, 10]:\n",
    "    df_features[f\"Return_Lag{lag}\"] = df_t[\"Return_1d\"].shift(lag)\n",
    "\n",
    "# Add random noise features (simulate spurious signals)\n",
    "for i in range(6, 21):  # creates Noise_6 … Noise_20\n",
    "    df_features[f\"Noise_{i}\"] = rng.normal(0, 0.01, len(df_t))\n",
    "\n",
    "# Drop missing values due to lagging\n",
    "df_features.dropna(inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Prepare Feature Matrix\n",
    "# --------------------------------------------------------------\n",
    "X_full = StandardScaler().fit_transform(df_features.values)\n",
    "n_samples, n_features = X_full.shape\n",
    "print(f\"Using {n_samples} samples and {n_features} features.\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Distance Concentration Computation\n",
    "# --------------------------------------------------------------\n",
    "dims = np.array([2, 5, 10, 20, 50])  # evaluate up to 50 dimensions\n",
    "n_points = min(500, n_samples)\n",
    "n_subsample = 60\n",
    "rel = []\n",
    "\n",
    "sub_idx = rng.choice(n_samples, size=n_points, replace=False)\n",
    "\n",
    "for d in dims:\n",
    "    X = X_full[sub_idx, :d]\n",
    "    mins, maxs = [], []\n",
    "    idx = rng.choice(n_points, size=n_subsample, replace=False)\n",
    "    \n",
    "    for i in idx:\n",
    "        diffs = X - X[i]\n",
    "        dists = np.sqrt(np.sum(diffs * diffs, axis=1))\n",
    "        dists = dists[dists > 0]\n",
    "        mins.append(dists.min())\n",
    "        maxs.append(dists.max())\n",
    "    \n",
    "    rel_contrast = (np.mean(maxs) - np.mean(mins)) / np.mean(mins)\n",
    "    rel.append(rel_contrast)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. Plot: Publication-Grade Visualization\n",
    "# --------------------------------------------------------------\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.plot(dims, rel, marker='o', color='tab:red', linewidth=2)\n",
    "plt.xlabel('Number of Features (Dimensionality)', fontsize=11)\n",
    "plt.ylabel('Relative Contrast', fontsize=11)\n",
    "plt.title(f'Distance Concentration — {ticker} ADR Feature Space', fontsize=12, weight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f740e9-837e-4401-b7b7-d0d8e32c8078",
   "metadata": {},
   "source": [
    "### Interpretation \n",
    "\n",
    "- As dimensionality grows from 2 → 50, the relative contrast collapses rapidly — confirming that distances become almost indistinguishable.\n",
    "\n",
    "- In this simulated high-dimensional financial feature space, “closeness” loses meaning — a hallmark of the curse of dimensionality.\n",
    "\n",
    "- This explains why classical distance-based algorithms (e.g., kNN, RBF-SVM, clustering) degrade as features expand without careful dimensionality control.\n",
    "\n",
    "- In practice, this motivates the use of representation learning or autoencoders to restore structure before applying predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26ec9c-657a-487b-a24a-5e63dd92e49e",
   "metadata": {},
   "source": [
    "## Polynomial feature growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da866d9a-4aee-41ca-9d2b-aa75ec3f8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3. Feature Explosion in Polynomial Models\n",
    "# ==============================================================\n",
    "\n",
    "import math\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Helper function: count polynomial feature combinations\n",
    "# --------------------------------------------------------------\n",
    "def poly_count(d, K):\n",
    "    \"\"\"\n",
    "    Computes the total number of polynomial features (excluding bias)\n",
    "    generated by a PolynomialFeatures transformer of degree K\n",
    "    given d original features.\n",
    "\n",
    "    Formula: sum_{k=1}^{K} C(d + k - 1, k)\n",
    "    \"\"\"\n",
    "    return sum(math.comb(d + k - 1, k) for k in range(1, K + 1))\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Define number of base features (d) and degrees (K)\n",
    "# --------------------------------------------------------------\n",
    "d = 20                        # number of original features (e.g., financial indicators)\n",
    "Ks = np.arange(1, 8)          # polynomial degrees 1 → 7\n",
    "counts = [poly_count(d, int(K)) for K in Ks]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Plot (log scale to visualize explosion)\n",
    "# --------------------------------------------------------------\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.plot(Ks, counts, marker='o', color='tab:blue', linewidth=2)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Polynomial Degree $K$', fontsize=11)\n",
    "plt.ylabel('Number of Features (log scale)', fontsize=11)\n",
    "plt.title(f'Polynomial Feature Explosion — d={d} Base Features', fontsize=12, weight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff2c34-3f51-4f11-b627-ac4b349e3ecd",
   "metadata": {},
   "source": [
    "## Interpretation \n",
    "\n",
    "- With only 20 original features, adding polynomial combinations up to degree 7 generates tens of millions of terms — even before training starts.\n",
    "\n",
    "- This illustrates the combinatorial explosion of features when classical ML tries to model non-linearity through explicit polynomial expansion.\n",
    "\n",
    "- Financial data, which is already noisy and correlated, becomes numerically unstable under such dimensional blow-ups.\n",
    "\n",
    "- Deep learning solves this by learning hierarchical non-linear mappings implicitly, without enumerating all interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7c840-d8ce-438b-ab90-5cc1efc0aac4",
   "metadata": {},
   "source": [
    "## Kernel scaling (n^2 memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59c600-548f-4bb8-a14c-d4ddbf91f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4. Kernel Scaling and Memory Growth (O(n²))\n",
    "# CIB ADR Context — sample scaling with realistic dataset sizes\n",
    "# ==============================================================\n",
    "\n",
    "# --- Start from actual CIB ADR dataset size\n",
    "n_real = len(df_t)\n",
    "n = np.array([n_real, 5000, 10000, 20000, 50000, 100000])\n",
    "mem_gb = (n.astype(float)**2 * 8) / (1024**3)  # float64 (8 bytes per entry)\n",
    "\n",
    "# --- Plot\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.plot(n, mem_gb, marker='o', color='tab:purple', linewidth=2)\n",
    "plt.xlabel('Number of Samples (n)', fontsize=11)\n",
    "plt.ylabel('Kernel Matrix Memory (GB)', fontsize=11)\n",
    "plt.title(f'Kernel Scaling — CIB ADR Context (O(n²) Memory Growth)', fontsize=12, weight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for size, mem in zip(n, mem_gb):\n",
    "    print(f\"{int(size):>7,} samples → {mem:>6.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d8534-b0e0-4c4c-9ee2-cb338be5e061",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "- Our CIB ADR dataset (around 2.5 k samples) fits easily in memory (around 0.05 GB).\n",
    "\n",
    "- But scaling to 50 k – 100 k samples (e.g., multi-asset or intraday data) would push kernel memory into 10–80 GB range — infeasible on most systems.\n",
    "\n",
    "- This shows why kernel SVMs and Gaussian Processes are rarely used in\n",
    "modern asset management pipelines with high-frequency or cross-sectional data.\n",
    "\n",
    "- Deep learning, by contrast, uses mini-batches and scales linearly in n, making it suitable for large time-series universes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6f595-ddf0-4c65-9b3f-c98bd31a4576",
   "metadata": {},
   "source": [
    "## Complexity vs depth (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb5036-d890-4697-b1ee-c060fdfcd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5. Decision Tree Complexity vs Depth (CIB ADR)\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Prepare Data\n",
    "# --------------------------------------------------------------\n",
    "# Use existing engineered features\n",
    "features = [\"Return_1d\", \"VolChange\", \"Return_5d\", \"Volatility_5d\"]\n",
    "X = df_t[features].values\n",
    "y = df_t[\"Target\"].values\n",
    "\n",
    "# Chronological split (70/30) to preserve time order\n",
    "split_idx = int(0.7 * len(df_t))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Train Decision Trees of increasing depth\n",
    "# --------------------------------------------------------------\n",
    "depths = range(1, 21)\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc.append(clf.score(X_train, y_train))\n",
    "    test_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Plot: Model Complexity vs Accuracy\n",
    "# --------------------------------------------------------------\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.plot(depths, train_acc, marker='o', label='Train Accuracy', color='tab:blue', linewidth=2)\n",
    "plt.plot(depths, test_acc, marker='o', label='Test Accuracy', color='tab:orange', linewidth=2)\n",
    "plt.xlabel('Decision Tree Max Depth', fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=11)\n",
    "plt.title(f'Model Complexity vs Depth — {ticker} ADR', fontsize=12, weight='bold')\n",
    "plt.legend(frameon=False)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print best depth\n",
    "best_depth = depths[np.argmax(test_acc)]\n",
    "print(f\"Best test accuracy: {max(test_acc):.3f} at depth = {best_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c35d8e-ec50-472f-adb9-6ebe4fc13239",
   "metadata": {},
   "source": [
    "### Interpretation (CIB ADR — Decision Tree Complexity)\n",
    "\n",
    "| Metric                 | Observation                                         | Meaning                                                            |\n",
    "| ---------------------- | --------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **Train Accuracy**     | Rises steadily from ~0.55 → 0.95 as depth increases | The tree is memorizing the data (variance ↑, bias ↓).              |\n",
    "| **Test Accuracy**      | Fluctuates around ~0.52 (random-guess level)        | The model overfits regime-specific noise; generalization ≈ chance. |\n",
    "| **Gap (Train − Test)** | Expands sharply with depth                          | Classic overfitting signature in non-stationary time-series data.  |\n",
    "\n",
    "---\n",
    "\n",
    "###  Insight\n",
    "\n",
    "* **Depth ≤ 3–5** → high bias, underfitting, but somewhat stable out-of-sample.\n",
    "* **Depth > 8** → low bias, *very high variance*; model captures historical noise, not structure.\n",
    "* This mirrors how **rule-based strategies** (e.g., decision trees on indicators) can fit past patterns but fail out-of-sample.\n",
    "* In market data, where the signal-to-noise ratio is < 1, adding model complexity rarely improves predictive accuracy — it often **amplifies regime-specific noise**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a878b-4bc2-40c5-bf76-5498a271b927",
   "metadata": {},
   "source": [
    "## Learning curve (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e05fb2-966a-45b3-be5c-2b79811a2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 6.  Learning Curve (Logistic Regression, CIB ADR)\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve, TimeSeriesSplit\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Prepare data\n",
    "# --------------------------------------------------------------\n",
    "df_t[\"Target\"] = (df_t[\"Return_1d\"].shift(-1) > 0).astype(int)\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "features = [\"Return_1d\", \"VolChange\", \"Return_5d\", \"Volatility_5d\"]\n",
    "X = df_t[features].values\n",
    "y = df_t[\"Target\"].values\n",
    "\n",
    "# Time-series-aware cross-validation (no leakage)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Model pipeline\n",
    "# --------------------------------------------------------------\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=2000)\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Compute learning curve\n",
    "# --------------------------------------------------------------\n",
    "sizes, tr, te = learning_curve(\n",
    "    model,\n",
    "    X, y,\n",
    "    cv=tscv,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Plot\n",
    "# --------------------------------------------------------------\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.plot(sizes, tr.mean(axis=1), marker='o', label='Train Accuracy', color='tab:blue', linewidth=2)\n",
    "plt.plot(sizes, te.mean(axis=1), marker='o', label='Test Accuracy', color='tab:orange', linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('Training Set Size', fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=11)\n",
    "plt.title(f'Learning Curve — Logistic Regression ({ticker} ADR)', fontsize=12, weight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f872ce-9331-4c91-ab90-a23ed5c2e549",
   "metadata": {},
   "source": [
    "### *Takeaway:*\n",
    "Even though Logistic Regression is simple, stable, and interpretable, it saturates quickly on financial data.\n",
    "Beyond a few hundred samples, performance plateaus because the feature space doesn’t represent underlying dynamics.\n",
    "This motivates moving to deep neural networks, which learn features automatically — capturing complex dependencies between returns, volatility, and volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b336e-a565-4e92-88c4-3677dc2d653c",
   "metadata": {},
   "source": [
    "## Residuals under mis-specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3fd70-fe7e-4504-9ff5-7075f747ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# ADR version — residuals of linear mis-specification\n",
    "# --------------------------------------------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Predictor: Volatility_5d, Target: next-day Return\n",
    "df_t[\"Target\"] = df_t[\"Return_1d\"].shift(-1)\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "X = df_t[[\"Volatility_5d\"]].values\n",
    "y = df_t[\"Target\"].values\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "resid = y - model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.scatter(df_t[\"Volatility_5d\"], resid, s=15, color='tab:red', alpha=0.7)\n",
    "plt.axhline(0, color='k', lw=1)\n",
    "plt.xlabel(\"5-Day Volatility\", fontsize=11)\n",
    "plt.ylabel(\"Residual\", fontsize=11)\n",
    "plt.title(f\"Residuals under Mis-Specification — {ticker} ADR\", fontsize=12, weight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2ca42-62cf-4734-a3a1-3a6976c1577f",
   "metadata": {},
   "source": [
    "### Plot Interpretation:\n",
    "- The residuals fan out as volatility increases, revealing heteroskedasticity and model mis-specification.\n",
    "- The linear model fails to capture the non-linear dependence between expected returns and volatility, producing structured residuals rather than white noise.\n",
    "- This provides a concrete visual motivation for non-linear or deep learning models in subsequent chapters, which can model these conditional dependencies directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
