{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cd32c1-24ed-456c-a903-458d78e58c2d",
   "metadata": {},
   "source": [
    "## Deep Learning Basics with PyTorch\n",
    "### Part III – Supervised Deep Learning in Practice\n",
    "## Chapter 11 — Deeper Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e71a67a-45d0-4e89-b2dc-7a1604c53a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch numpy matplotlib  # install dependencies if needed\n",
    "import matplotlib.pyplot as plt  # plotting utilities\n",
    "import numpy as np  # numerical helpers\n",
    "import torch  # tensor library\n",
    "import torch.nn.functional as F  # functional conv helpers\n",
    "from IPython import get_ipython  # notebook runtime access\n",
    "plt.style.use(\"seaborn-v0_8\")  # consistent styling\n",
    "get_ipython().run_line_magic(\"config\", \"InlineBackend.figure_format = 'retina'\")  # crisp notebook figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce543a4a-f61d-44d3-b2a1-09cf15871a64",
   "metadata": {},
   "source": [
    "## Convolution (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7bae04-80f5-422f-a918-bd95712c2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (1, 1, 6, 6)\n",
      "output shape: (1, 1, 4, 4)\n",
      "output map:\n",
      " tensor([[ 0.,  1.,  1.,  0.],\n",
      "        [ 1., -2., -2.,  1.],\n",
      "        [ 1., -2., -2.,  1.],\n",
      "        [ 0.,  1.,  1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 1, 6, 6)  # blank canvas\n",
    "x[:, :, 2:4, 2:4] = 1.0  # add bright 2x2 square in the centre\n",
    "kernel = torch.tensor([[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]]).view(1, 1, 3, 3)  # laplacian-style filter\n",
    "y = F.conv2d(x, kernel, stride=1, padding=0)  # valid convolution\n",
    "print(\"input shape:\", tuple(x.shape))  # confirm input shape\n",
    "print(\"output shape:\", tuple(y.shape))  # confirm output shape\n",
    "print(\"output map:\\n\", y.squeeze())  # display response values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e14728-793b-47e1-bed4-51c892bda7ae",
   "metadata": {},
   "source": [
    "## Padding vs stride (shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ee6d15-242b-4bc9-a421-d8290a445e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no padding: (3, 3)\n",
      "padding=1: (5, 5)\n",
      "stride=2: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.randn(1, 1, 5, 5)  # random 5x5 input\n",
    "kernel = torch.ones(1, 1, 3, 3)  # 3x3 averaging kernel\n",
    "no_pad = F.conv2d(sample, kernel, stride=1, padding=0)  # valid conv\n",
    "same_pad = F.conv2d(sample, kernel, stride=1, padding=1)  # keep size\n",
    "stride_two = F.conv2d(sample, kernel, stride=2, padding=0)  # stride-2 conv\n",
    "print(\"no padding:\", tuple(no_pad.shape[-2:]))  # show spatial dims\n",
    "print(\"padding=1:\", tuple(same_pad.shape[-2:]))  # show same-size dims\n",
    "print(\"stride=2:\", tuple(stride_two.shape[-2:]))  # show downsampled dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1cc724-b878-4a4b-a5cf-82e49b41e735",
   "metadata": {},
   "source": [
    "## Tiny CNN (shape tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0212d84-75b3-4e40-b113-220b9d9ece75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (4, 1, 28, 28)\n",
      "features: (4, 16, 7, 7)\n",
      "logits: (4, 10)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn  # neural network modules\n",
    "class TinyCNN(nn.Module):  # simple two-block CNN\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(  # feature extractor\n",
    "            nn.Conv2d(1, 8, 3, padding=1),  # conv block 1 keeps resolution\n",
    "            nn.ReLU(),  # nonlinearity\n",
    "            nn.MaxPool2d(2),  # downsample by 2\n",
    "            nn.Conv2d(8, 16, 3, padding=1),  # conv block 2\n",
    "            nn.ReLU(),  # nonlinearity\n",
    "            nn.MaxPool2d(2),  # downsample by 2 again\n",
    "        )\n",
    "        self.classifier = nn.Sequential(  # linear head\n",
    "            nn.Flatten(),  # flatten to (B, features)\n",
    "            nn.Linear(16 * 7 * 7, 10),  # map to logits\n",
    "        )\n",
    "    def forward(self, x):  # forward propagation\n",
    "        x = self.features(x)  # apply conv blocks\n",
    "        return self.classifier(x)  # project to class scores\n",
    "model = TinyCNN()  # instantiate network\n",
    "sample = torch.randn(4, 1, 28, 28)  # mini-batch of images\n",
    "feats = model.features(sample)  # extract feature maps\n",
    "logits = model(sample)  # run full network\n",
    "print(\"input:\", tuple(sample.shape))  # show input shape\n",
    "print(\"features:\", tuple(feats.shape))  # show feature map shape\n",
    "print(\"logits:\", tuple(logits.shape))  # show classifier output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
