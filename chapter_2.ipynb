{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2051b719-02c4-416a-b1bb-dc145c08d3bc",
   "metadata": {},
   "source": [
    "# *Deep Learning Basics with PyTorch*\n",
    "# Part I — Foundations of Machine Learning\n",
    "## Chapter 2: Data, Features, and Representations\n",
    "In this chapter, we reconstructed the classic \"Iris\" ML workflow using financial data.\n",
    "Each step — feature creation, visualization, model fitting, and boundary inspection —\n",
    "builds intuition for how machine learning interprets patterns in markets.\n",
    "\n",
    "The same pipeline underpins deep learning models, which we will explore in the next chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23202bc2-1b90-489c-8156-9bb71c2933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5bc98-1272-4fb7-9cc5-ab9431db8960",
   "metadata": {},
   "source": [
    "## Building a Machine Learning Dataset from ADR Market Data\n",
    "We start by transforming raw price and volume series for a chosen ADR (e.g. GGAL) into daily returns and volume changes — the simplest features capturing market direction and liquidity shifts.\n",
    "A binary target encodes whether the day closed up (1) or down (0).\n",
    "This preprocessing step mirrors classic feature extraction in ML, but applied to real market micro-structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5711be-40d1-43f6-9908-1db92868413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and prepare ADR data\n",
    "df = pd.read_csv(\"adr_prices_and_vol.csv\", parse_dates=[\"Date\"])\n",
    "ticker = \"GGAL\"\n",
    "\n",
    "# Select columns and drop missing values\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\", f\"{ticker}_Volume\"]].dropna().copy()\n",
    "\n",
    "# Create simple features (returns and volume change)\n",
    "df_t[\"Return_1d\"] = df_t[f\"{ticker}_Price\"].pct_change()\n",
    "df_t[\"VolChange\"] = df_t[f\"{ticker}_Volume\"].pct_change()\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# Create a simple binary target: 1 = Up day, 0 = Down day\n",
    "df_t[\"Target\"] = (df_t[\"Return_1d\"].shift(-1) > 0).astype(int)\n",
    "\n",
    "# --- Define global feature matrix and target vector ---\n",
    "features = [\"Return_1d\", \"VolChange\"]\n",
    "X = df_t[features].values\n",
    "y = df_t[\"Target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9fb08-e8e0-48cf-b4d1-f26c22f1007c",
   "metadata": {},
   "source": [
    "## Load and Visualize Financial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee7d4a-2208-455c-a6ba-69cb3caa750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization (analogous to the Iris scatter)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(\n",
    "    df_t.loc[df_t[\"Target\"] == 0, \"Return_1d\"],\n",
    "    df_t.loc[df_t[\"Target\"] == 0, \"VolChange\"],\n",
    "    label=\"Down Day\", marker=\"o\", alpha=0.6\n",
    ")\n",
    "plt.scatter(\n",
    "    df_t.loc[df_t[\"Target\"] == 1, \"Return_1d\"],\n",
    "    df_t.loc[df_t[\"Target\"] == 1, \"VolChange\"],\n",
    "    label=\"Up Day\", marker=\"^\", alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Daily Return\")\n",
    "plt.ylabel(\"Volume Change\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(f\"{ticker} — Feature Space: Return vs Volume\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e3fec-082e-4d8b-9ce0-e0a76de77747",
   "metadata": {},
   "source": [
    "- This scatterplot shows how price momentum (returns) and trading activity (volume change) interact.\n",
    "- Up-days cluster differently from down-days, suggesting a weak but learnable pattern.\n",
    "- Visualization remains the most intuitive way to check separability before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8cf0b-a339-448f-8b72-2529e2291853",
   "metadata": {},
   "source": [
    "## Exploring Alternative Feature Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21061320-0af1-4891-a54e-1a506183ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature construction (5-day return and volatility)\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\"]].dropna().copy()\n",
    "df_t[\"Return_5d\"] = df_t[f\"{ticker}_Price\"].pct_change(5)\n",
    "df_t[\"Volatility_5d\"] = df_t[\"Return_5d\"].rolling(5).std()\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# --- Target: 1 = Up 5d return, 0 = Down 5d return\n",
    "df_t[\"Target\"] = (df_t[\"Return_5d\"] > 0).astype(int)\n",
    "\n",
    "# --- Visualization\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(\n",
    "    df_t.loc[df_t[\"Target\"] == 0, \"Return_5d\"],\n",
    "    df_t.loc[df_t[\"Target\"] == 0, \"Volatility_5d\"],\n",
    "    label=\"5-Day Down Period\", marker=\"o\", alpha=0.6\n",
    ")\n",
    "plt.scatter(\n",
    "    df_t.loc[df_t[\"Target\"] == 1, \"Return_5d\"],\n",
    "    df_t.loc[df_t[\"Target\"] == 1, \"Volatility_5d\"],\n",
    "    label=\"5-Day Up Period\", marker=\"^\", alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"5-Day Return\")\n",
    "plt.ylabel(\"5-Day Rolling Volatility\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(f\"{ticker} — Alternative Feature Projection\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324d1bf-10e7-40c2-9c6d-5f8b10867eae",
   "metadata": {},
   "source": [
    "- Feature engineering changes what patterns become visible.\n",
    "- Aggregating over five days smooths noise and introduces volatility as a second-order feature.\n",
    "- Here we see whether multi-day behavior offers better separability — a precursor to using richer\n",
    "temporal features or deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6944f-35ff-408d-9e9d-efc41bd7c685",
   "metadata": {},
   "source": [
    "## Train a Scaler + Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4bee5-7c7f-4f45-a7b1-01fda3fb5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train/Test split, scaling, and logistic regression ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, display_labels=[\"Down\", \"Up\"], cmap=\"Blues\"\n",
    ")\n",
    "plt.title(f\"{ticker} — Logistic Regression (Up vs Down Days)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da03989-20c7-4675-8f6c-ecde32d71f37",
   "metadata": {},
   "source": [
    "We apply a minimal ML pipeline — standardizing inputs ensures the model isn’t biased by scale differences between returns and volume changes. A logistic regression then estimates the probability of an up-day. Accuracy and the confusion matrix quantify how often the model gets direction right; interpret both rather than celebrating a single metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a46acb-4e80-4c5f-99b9-a6d3b48b8fe5",
   "metadata": {},
   "source": [
    "## Decision Boundary in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1ec22-328e-4dea-ad74-640728e4fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit model on entire dataset for visualization ---\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# --- Create meshgrid across feature space ---\n",
    "xmin, xmax = X[:, 0].min() - 0.02, X[:, 0].max() + 0.02\n",
    "ymin, ymax = X[:, 1].min() - 0.02, X[:, 1].max() + 0.02\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xmin, xmax, 300),\n",
    "    np.linspace(ymin, ymax, 300)\n",
    ")\n",
    "\n",
    "# --- Predict class across grid ---\n",
    "zz = pipe.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "# --- Plot boundary and data ---\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contourf(xx, yy, zz, levels=[-0.5, 0.5, 1.5], cmap='coolwarm', alpha=0.2)\n",
    "\n",
    "plt.scatter(\n",
    "    X[y == 0, 0], X[y == 0, 1],\n",
    "    marker='o', label='Down Day', alpha=0.6\n",
    ")\n",
    "plt.scatter(\n",
    "    X[y == 1, 0], X[y == 1, 1],\n",
    "    marker='^', label='Up Day', alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Daily Return\")\n",
    "plt.ylabel(\"Volume Change\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(f\"{ticker} — Logistic Regression Decision Boundary\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23532d6-cfce-4b58-9040-f2575bb2c661",
   "metadata": {},
   "source": [
    "- The decision boundary divides the 2-D feature space into regions the classifier labels as “Up” or “Down.”\n",
    "- In finance, such a boundary can be interpreted as a linear trading signal frontier — a simple function of return and liquidity change.\n",
    "- Inspect whether up-days sit mostly inside the predicted region; deviations hint at noise or regime shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828a747-51c9-4c1b-80a6-759dfd29afab",
   "metadata": {},
   "source": [
    "## Quick Checks\n",
    "\n",
    "Before trusting model metrics, always verify the basics:\n",
    "\n",
    "- **Class Balance:**  \n",
    "  After splitting, ensure the class proportions are similar in train and test sets.  \n",
    "  Use `np.bincount(y_train)` and `np.bincount(y_test)` to confirm.\n",
    "\n",
    "- **Proper Scaling:**  \n",
    "  Confirm scaling occurs *within* the pipeline — the scaler should be fit only on the training data,  \n",
    "  then applied to the test data automatically. This avoids data leakage.\n",
    "\n",
    "- **Feature Sensitivity:**  \n",
    "  Try swapping features (e.g., `Return_5d` and `Volatility_5d` instead of `Return_1d` and `VolChange`).  \n",
    "  The change in accuracy or decision boundary sharpness will show how each feature contributes  \n",
    "  to separability — just as the Iris dataset demonstrates petal vs sepal contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ade09-cfb0-4818-961a-72fef497346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sanity Checks ---\n",
    "print(\"Train class balance:\", np.bincount(y_train))\n",
    "print(\"Test class balance :\", np.bincount(y_test))\n",
    "\n",
    "# Confirm scaling within pipeline\n",
    "print(\"\\nPipeline steps:\", pipe.named_steps.keys())\n",
    "print(\"Scaler fitted on training data only:\", hasattr(pipe.named_steps['standardscaler'], 'scale_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c5176-fa9a-404c-a1f3-d8f3f87645f8",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "These exercises mirror the structure of the original Chapter 2 but use real financial data instead of the Iris dataset.  \n",
    "Each question is designed to deepen your understanding of how features, scaling, and model geometry affect classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab4a76-0102-4286-add2-05e41c6716e7",
   "metadata": {},
   "source": [
    "### Excercise 1\n",
    "> **1. Try a different pair of financial features**  \n",
    "Replace *daily return* and *volume change* with an alternative feature pair — for example,  5-day return and 5-day volatility — and compare classification accuracy.  \n",
    "How does the model’s separability and decision boundary change when using multi-day (smoothed) features instead of single-day dynamics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5d6a0-4b21-4aee-b438-14b635ddc1a7",
   "metadata": {},
   "source": [
    "1) Build alternative features (5-day horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e318733-65c5-4f4b-8ea4-597ee1bc874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Alternative feature construction (5-day horizon) ---\n",
    "df_alt = df[[\"Date\", f\"{ticker}_Price\"]].dropna().copy()\n",
    "df_alt = df_alt.sort_values(\"Date\")\n",
    "\n",
    "# Features\n",
    "df_alt[\"Return_5d\"]     = df_alt[f\"{ticker}_Price\"].pct_change(5)\n",
    "df_alt[\"Volatility_5d\"] = df_alt[\"Return_5d\"].rolling(5).std()\n",
    "\n",
    "# Target: 1 if next 5-day return > 0, else 0\n",
    "df_alt[\"NextReturn_5d\"] = df_alt[\"Return_5d\"].shift(-5)\n",
    "df_alt[\"Target\"]        = (df_alt[\"NextReturn_5d\"] > 0).astype(int)\n",
    "\n",
    "# Clean NA rows introduced by rolling/shift\n",
    "df_alt = df_alt.dropna(subset=[\"Return_5d\", \"Volatility_5d\", \"NextReturn_5d\"])\n",
    "\n",
    "features_alt = [\"Return_5d\", \"Volatility_5d\"]\n",
    "X_alt = df_alt[features_alt].values\n",
    "y_alt = df_alt[\"Target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73fe38-d44d-4456-a7dc-c588ccb79ff8",
   "metadata": {},
   "source": [
    "2) Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf5698-3426-4b58-9bcd-b9ffb722a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended: chronological split (avoids look-ahead bias)\n",
    "split_date = \"2020-01-01\"\n",
    "train_alt  = df_alt[df_alt[\"Date\"] < split_date]\n",
    "test_alt   = df_alt[df_alt[\"Date\"] >= split_date]\n",
    "\n",
    "Xtr_alt = train_alt[features_alt].values\n",
    "ytr_alt = train_alt[\"Target\"].values\n",
    "Xte_alt = test_alt[features_alt].values\n",
    "yte_alt = test_alt[\"Target\"].values\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pipe_alt = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe_alt.fit(Xtr_alt, ytr_alt)\n",
    "yp_alt = pipe_alt.predict(Xte_alt)\n",
    "\n",
    "acc_alt = accuracy_score(yte_alt, yp_alt)\n",
    "print(f\"Accuracy (5d features, chrono split): {acc_alt:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(yte_alt, yp_alt, display_labels=[\"Down\",\"Up\"], cmap=\"Blues\")\n",
    "plt.title(f\"{ticker} — Confusion Matrix (5d Return & 5d Volatility)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2411e-d356-4156-a63c-6ecaa20a1f12",
   "metadata": {},
   "source": [
    "3) Decision boundary in 2D (with 5-day features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88ab27-0c4f-4dad-87f9-4992e5fba086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on all data for a smooth boundary picture (ok for visualization only)\n",
    "pipe_vis = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe_vis.fit(X_alt, y_alt)\n",
    "\n",
    "xmin, xmax = X_alt[:, 0].min() - 0.01, X_alt[:, 0].max() + 0.01\n",
    "ymin, ymax = X_alt[:, 1].min() - 0.01, X_alt[:, 1].max() + 0.01\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xmin, xmax, 300),\n",
    "    np.linspace(ymin, ymax, 300)\n",
    ")\n",
    "zz = pipe_vis.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.contourf(xx, yy, zz, levels=[-0.5, 0.5, 1.5], alpha=0.2, cmap=\"coolwarm\")\n",
    "plt.scatter(X_alt[y_alt==0, 0], X_alt[y_alt==0, 1], marker=\"o\", alpha=0.6, label=\"Down\")\n",
    "plt.scatter(X_alt[y_alt==1, 0], X_alt[y_alt==1, 1], marker=\"^\", alpha=0.6, label=\"Up\")\n",
    "plt.xlabel(\"5-Day Return\")\n",
    "plt.ylabel(\"5-Day Volatility\")\n",
    "plt.title(f\"{ticker} — Decision Boundary (5d Return vs 5d Volatility)\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb8c12-f23e-4962-b204-b6833f9fac20",
   "metadata": {},
   "source": [
    "### Exercise 1 — Results and Interpretation\n",
    "\n",
    "**Observation**\n",
    "\n",
    "Using 5-day return and 5-day volatility as features produces a smoother decision surface,  \n",
    "but accuracy drops to around **0.46**, as seen in the confusion matrix.  \n",
    "The model tends to classify most periods as “Up,” indicating that the aggregated 5-day features  \n",
    "blur short-term reversals and introduce overlap between classes.\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "- **Smoothing effect:** Multi-day aggregation filters out noise but also reduces the number of distinct samples,  \n",
    "  weakening short-term discriminative power.  \n",
    "- **Feature correlation:** 5-day return and volatility are not orthogonal—higher volatility can occur in both  \n",
    "  rising and falling markets, limiting linear separability.  \n",
    "- **Financial interpretation:** The logistic boundary still captures broad market regimes  \n",
    "  (high-volatility clusters vs calm uptrends), yet short-horizon predictability is low.\n",
    "\n",
    "**Takeaway**\n",
    "\n",
    "> Smoothing features can stabilize patterns and reveal structure,  \n",
    "> but it often sacrifices predictive resolution — a key trade-off in financial ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a775af-becc-4bcb-8336-b58fcc3f5280",
   "metadata": {},
   "source": [
    "### Excercise 2\n",
    "> **2. Swap Logistic Regression for a Linear SVM**  \n",
    "Replace `LogisticRegression` with `SVC(kernel='linear')` from `sklearn.svm`.  \n",
    "Compare how the boundary orientation and margin width differ between the two models.  \n",
    "Which classifier generalizes better on your ADR data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743bc7e-949a-4c62-9612-f55252b05308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Exercise 2 — Linear SVM vs Logistic Regression (simple version)\n",
    "# ==============================================================\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Load and prepare ADR data\n",
    "df = pd.read_csv(\"adr_prices_and_vol.csv\", parse_dates=[\"Date\"])\n",
    "ticker = \"GGAL\"\n",
    "\n",
    "# --- Build features and target (1-day return + volume change)\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\", f\"{ticker}_Volume\"]].dropna().copy()\n",
    "df_t[\"Return_1d\"] = df_t[f\"{ticker}_Price\"].pct_change()\n",
    "df_t[\"VolChange\"] = df_t[f\"{ticker}_Volume\"].pct_change()\n",
    "df_t.dropna(inplace=True)\n",
    "df_t[\"Target\"] = (df_t[\"Return_1d\"] > 0).astype(int)\n",
    "\n",
    "# --- Split chronologically (to respect time order)\n",
    "split_date = \"2020-01-01\"\n",
    "train = df_t[df_t[\"Date\"] < split_date]\n",
    "test  = df_t[df_t[\"Date\"] >= split_date]\n",
    "\n",
    "X_train, y_train = train[[\"Return_1d\", \"VolChange\"]].values, train[\"Target\"].values\n",
    "X_test,  y_test  = test[[\"Return_1d\", \"VolChange\"]].values,  test[\"Target\"].values\n",
    "\n",
    "# --- Train linear SVM\n",
    "pipe = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=1.0))\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# --- Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy (Linear SVM, chrono split): {acc:.3f}\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=[\"Down\",\"Up\"], cmap=\"Blues\")\n",
    "plt.title(f\"{ticker} — Confusion Matrix (Linear SVM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Decision Boundary\n",
    "X, y = df_t[[\"Return_1d\",\"VolChange\"]].values, df_t[\"Target\"].values\n",
    "pipe_vis = make_pipeline(StandardScaler(), SVC(kernel=\"linear\", C=1.0))\n",
    "pipe_vis.fit(X, y)\n",
    "\n",
    "xmin, xmax = X[:,0].min()-0.02, X[:,0].max()+0.02\n",
    "ymin, ymax = X[:,1].min()-0.02, X[:,1].max()+0.02\n",
    "xx, yy = np.meshgrid(np.linspace(xmin,xmax,300), np.linspace(ymin,ymax,300))\n",
    "zz = pipe_vis.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.contourf(xx, yy, zz, levels=[-0.5,0.5,1.5], cmap=\"coolwarm\", alpha=0.25)\n",
    "plt.scatter(X[y==0,0], X[y==0,1], marker=\"o\", alpha=0.6, label=\"Down\")\n",
    "plt.scatter(X[y==1,0], X[y==1,1], marker=\"^\", alpha=0.6, label=\"Up\")\n",
    "plt.xlabel(\"Daily Return\"); plt.ylabel(\"Volume Change\")\n",
    "plt.title(f\"{ticker} — Decision Boundary (Linear SVM)\")\n",
    "plt.legend(frameon=False); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374149b-53ca-444f-a87e-ba672e3a50dd",
   "metadata": {},
   "source": [
    "- Linear SVM finds the maximum-margin hyperplane separating up vs down days in the feature space of daily return and volume change.\n",
    "- The decision boundary is nearly vertical — meaning return direction dominates classification, while volume change contributes little discriminative power.\n",
    "- The near-perfect accuracy (~0.999) reflects that this simplified sample is linearly separable given the strong asymmetry of daily returns.\n",
    "- In real financial data, such perfect separability is rare — adding noise, transaction costs, or regime shifts would sharply reduce performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e034d96-777b-4682-9f9a-db165556ff61",
   "metadata": {},
   "source": [
    "## Excercise 3\n",
    "> **3. Regularization and Validation**  \n",
    "Add a validation split and tune the regularization parameter `C` for logistic regression.  \n",
    "Observe how increasing `C` (weaker regularization) affects overfitting and boundary sharpness.  \n",
    "Do you notice instability when the data becomes noisier or class imbalance increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd1856-43c1-4e44-af60-e1970dc291b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load & prepare ADR data\n",
    "df = pd.read_csv(\"adr_prices_and_vol.csv\", parse_dates=[\"Date\"])\n",
    "ticker = \"GGAL\"\n",
    "\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\", f\"{ticker}_Volume\"]].dropna().copy()\n",
    "df_t[\"Return_1d\"] = df_t[f\"{ticker}_Price\"].pct_change()\n",
    "df_t[\"VolChange\"] = df_t[f\"{ticker}_Volume\"].pct_change()\n",
    "df_t.dropna(inplace=True)\n",
    "df_t[\"Target\"] = (df_t[\"Return_1d\"] > 0).astype(int)\n",
    "\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target\"].values\n",
    "\n",
    "# --- Chronological split: train / validation / test\n",
    "split1, split2 = int(0.6 * len(X)), int(0.8 * len(X))\n",
    "X_train, X_val, X_test = X[:split1], X[split1:split2], X[split2:]\n",
    "y_train, y_val, y_test = y[:split1], y[split1:split2], y[split2:]\n",
    "\n",
    "# --- Grid search over regularization strength\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "for C in C_values:\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression(C=C, max_iter=1000))\n",
    "    pipe.fit(X_train, y_train)\n",
    "    train_acc.append(pipe.score(X_train, y_train))\n",
    "    val_acc.append(pipe.score(X_val, y_val))\n",
    "\n",
    "# --- Plot performance vs C\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.semilogx(C_values, train_acc, marker='o', label='Train Accuracy')\n",
    "plt.semilogx(C_values, val_acc, marker='s', label='Validation Accuracy')\n",
    "plt.xlabel(\"Regularization Strength (C)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"{ticker} — Regularization and Validation (LogReg)\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87156106-0e3c-4149-a03c-2a03e5c5f669",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "- The plot illustrates how regularization strength (C) shapes model behavior: small C values (stronger regularization) produce smoother, more conservative decision boundaries, while large C values (weaker regularization) let the model fit the training data more tightly.\n",
    "- Both training and validation accuracies converge near 1.0, showing the feature pair (Return_1d, VolChange) is already linearly separable — additional flexibility offers little benefit.\n",
    "- In practice, real financial features are far noisier; overly weak regularization can cause unstable coefficients and spurious separability, especially under regime shifts.\n",
    "- The key takeaway: moderate regularization improves robustness, helping the model generalize when market dynamics evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2726d-a50e-4d59-a294-94306bedfe43",
   "metadata": {},
   "source": [
    "## Bonus Challenge:\n",
    "- Extend the binary target to a 3-class setup (e.g., Down > –1%, Flat ±1%, Up > +1%).  \n",
    "- Test whether a linear model can still learn meaningful boundaries or if non-linear methods (SVM with RBF kernel, or shallow neural networks) start to outperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842c55f-a273-4fa7-9ca8-35616672ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Bonus Challenge — Multi-Class Setup\n",
    "# ==============================================================\n",
    "# --- Load and prepare data\n",
    "df = pd.read_csv(\"adr_prices_and_vol.csv\", parse_dates=[\"Date\"])\n",
    "ticker = \"GGAL\"\n",
    "\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\", f\"{ticker}_Volume\"]].dropna().copy()\n",
    "df_t[\"Return_1d\"] = df_t[f\"{ticker}_Price\"].pct_change()\n",
    "df_t[\"VolChange\"] = df_t[f\"{ticker}_Volume\"].pct_change()\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# --- Define 3-class target based on daily return thresholds\n",
    "bins = [-np.inf, -0.01, 0.01, np.inf]\n",
    "labels = [0, 1, 2]  # 0=Down, 1=Flat, 2=Up\n",
    "df_t[\"Target3\"] = np.digitize(df_t[\"Return_1d\"], bins) - 1\n",
    "\n",
    "# --- Features and labels\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target3\"].values\n",
    "\n",
    "# --- Chronological split (time-series friendly)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# --- Multiclass Logistic Regression (default = multinomial)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Accuracy and confusion matrix\n",
    "acc = accuracy_score(y_test, pipe.predict(X_test))\n",
    "print(f\"Accuracy (3-class): {acc:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, pipe.predict(X_test),\n",
    "    display_labels=[\"Down\", \"Flat\", \"Up\"],\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "plt.title(f\"{ticker} — Confusion Matrix (3-class LogReg)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183378b-dae5-43bd-a167-e8ccb025270f",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "- The confusion matrix shows the model accurately classifies strong up/down days, while most uncertainty concentrates in the flat regime, where daily returns hover near zero.\n",
    "- The 0.99 accuracy indicates that the majority of periods fall clearly into “Up” or “Down,” confirming the market’s directional bias dominates over noise in this subset.\n",
    "- However, the “Flat” cluster highlights the limitation of linear separability — even a simple multinomial logistic regression struggles to cleanly divide low-volatility days.\n",
    "- This behavior mirrors real trading challenges: transitions between bull, neutral, and bear micro-regimes rarely align with a single linear boundary.\n",
    "- The next logical step (and what Part II of the book explores) is to introduce non-linear representations — kernels or neural networks — to better capture these subtle transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c345cb-da64-4536-828a-ae92c147b350",
   "metadata": {},
   "source": [
    "## Non-linear SVM (RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7fc4bb-bb85-4a0c-9e41-7f8e9cd21993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Bonus Extension — Non-Linear SVM (RBF Kernel)\n",
    "# ==============================================================\n",
    "\n",
    "# --- Feature matrix and target from 3-class setup\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target3\"].values\n",
    "\n",
    "# --- Chronological split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# --- Pipeline with StandardScaler + SVM (RBF kernel)\n",
    "pipe_rbf = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=\"scale\", C=1.0))\n",
    "pipe_rbf.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate performance\n",
    "y_pred = pipe_rbf.predict(X_test)\n",
    "acc_rbf = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy (3-class RBF SVM): {acc_rbf:.3f}\")\n",
    "\n",
    "# --- Confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred,\n",
    "                                        display_labels=[\"Down\", \"Flat\", \"Up\"],\n",
    "                                        cmap=\"viridis\")\n",
    "plt.title(f\"{ticker} — Confusion Matrix (3-class RBF SVM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Decision boundary visualization\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X[:, 0].min(), X[:, 0].max(), 200),\n",
    "    np.linspace(X[:, 1].min(), X[:, 1].max(), 200)\n",
    ")\n",
    "zz = pipe_rbf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.contourf(xx, yy, zz, cmap=\"coolwarm\", alpha=0.25)\n",
    "for cls, marker, label in [\n",
    "    (0, 'o', 'Down'),\n",
    "    (1, 's', 'Flat'),\n",
    "    (2, '^', 'Up')\n",
    "]:\n",
    "    idx = y == cls\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], marker=marker, label=label, alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Daily Return\")\n",
    "plt.ylabel(\"Volume Change\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(f\"{ticker} — Decision Boundary (3-class RBF SVM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351ee4c-ea1e-46ef-9c40-ccbfb888d179",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "- The RBF SVM introduces curvature into the separating surfaces, adapting to the denser “Flat” region where class overlap occurs.\n",
    "- Accuracy (≈ 0.99) remains close to the linear model, but the decision boundary now flexes along the data distribution, creating smoother transitions between “Down,” “Flat,” and “Up” regimes.\n",
    "- The shaded regions in the plot highlight non-linear separability: the SVM’s kernel implicitly maps features into a higher-dimensional space, allowing for complex shapes without explicitly creating new variables.\n",
    "- This subtle flexibility marks the bridge between classical ML and representation learning — the very motivation for neural networks explored in Part II: Deep Learning Foundations.\n",
    "- In real market data, where structure is rarely linear, such adaptive boundaries are often the first step toward feature hierarchies that deep models learn automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
