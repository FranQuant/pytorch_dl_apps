{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ba6a08-6379-4de8-bd30-8eee4dce757f",
   "metadata": {},
   "source": [
    "# Deep Learning Basics with PyTorch\n",
    "## Part II — Neural Networks and PyTorch Basics\n",
    "## Chapter 7 — Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f48a8e-0f8c-4dcb-9359-407c8b31ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install torch numpy matplotlib scikit-learn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e1815-7e6f-425d-99e4-ee82f66b713a",
   "metadata": {},
   "source": [
    "## Minimal MLP and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb1720b-2033-4e53-9f2d-0c9b678f7b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866666555404663"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data\n",
    "torch.manual_seed(0)\n",
    "X, y = make_moons(n_samples=600, noise=0.25, random_state=0)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
    "X_te = torch.tensor(X_te, dtype=torch.float32)\n",
    "y_tr = torch.tensor(y_tr, dtype=torch.long)\n",
    "y_te = torch.tensor(y_te, dtype=torch.long)\n",
    "\n",
    "# Two-layer MLP params (leaf tensors with grads)\n",
    "W1 = torch.randn(2, 16, requires_grad=True)  # layer 1 weights\n",
    "b1 = torch.zeros(16, requires_grad=True)      # layer 1 bias\n",
    "W2 = torch.randn(16, 2, requires_grad=True)  # layer 2 weights\n",
    "b2 = torch.zeros(2, requires_grad=True)      # layer 2 bias\n",
    "\n",
    "# Light init scaling without tracking gradients\n",
    "with torch.no_grad():\n",
    "    W1.mul_(0.5)\n",
    "    W2.mul_(0.5)\n",
    "\n",
    "def forward(X):\n",
    "    h = torch.relu(X @ W1 + b1)  # hidden activations\n",
    "    return h @ W2 + b2\n",
    "\n",
    "# Manual SGD loop\n",
    "for _ in range(300):\n",
    "    logits = forward(X_tr)\n",
    "    loss = F.cross_entropy(logits, y_tr)\n",
    "    for p in (W1, b1, W2, b2):\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in (W1, b1, W2, b2):\n",
    "            p -= 0.1 * p.grad\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "with torch.no_grad():\n",
    "    acc = float((forward(X_te).argmax(1) == y_te).float().mean())\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd46cd-0612-44ae-8f95-9f415d6c09fc",
   "metadata": {},
   "source": [
    "## Optimizers: SGD vs Adam (quick check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69ec3fa-93d8-45f3-864e-110d7a8a26cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8933333158493042, 0.9133333563804626)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run(opt_name, lr):\n",
    "    # Fresh params each run\n",
    "    W1 = torch.randn(2, 16, requires_grad=True)\n",
    "    b1 = torch.zeros(16, requires_grad=True)\n",
    "    W2 = torch.randn(16, 2, requires_grad=True)\n",
    "    b2 = torch.zeros(2, requires_grad=True)\n",
    "    with torch.no_grad():\n",
    "        W1.mul_(0.5)\n",
    "        W2.mul_(0.5)\n",
    "    params = [W1, b1, W2, b2]\n",
    "    optimizer = (\n",
    "        torch.optim.SGD(params, lr=lr)\n",
    "        if opt_name == 'sgd'\n",
    "        else torch.optim.Adam(params, lr=lr)\n",
    "    )\n",
    "    for _ in range(200):\n",
    "        logits = torch.relu(X_tr @ W1 + b1) @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, y_tr)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        logits_te = torch.relu(X_te @ W1 + b1) @ W2 + b2\n",
    "        acc = float((logits_te.argmax(1) == y_te).float().mean())\n",
    "    return acc\n",
    "\n",
    "sgd_acc = run('sgd', 0.1)\n",
    "adam_acc = run('adam', 0.01)\n",
    "sgd_acc, adam_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
