{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2051b719-02c4-416a-b1bb-dc145c08d3bc",
   "metadata": {},
   "source": [
    "# *Deep Learning Basics with PyTorch*\n",
    "# Part I — Foundations of Machine Learning\n",
    "## Chapter 3 — Basic Models in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ed197-64b6-4c41-986e-267b36f2a28e",
   "metadata": {},
   "source": [
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f871a-500b-4f98-b345-d7fe7ae6d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')  # plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb3316-8ef4-463a-9e79-73f62baa4071",
   "metadata": {},
   "source": [
    "## Data and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d674c-ee79-4afc-96aa-d3e955091eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1. Load ADR Dataset and Build Features (Chronologically Indexed)\n",
    "# ==============================================================\n",
    "\n",
    "# Load ADR data\n",
    "df = pd.read_csv(\"adr_prices_and_vol.csv\", parse_dates=[\"Date\"])\n",
    "ticker = \"CIB\"\n",
    "\n",
    "# Select relevant columns and drop missing data\n",
    "df_t = df[[\"Date\", f\"{ticker}_Price\", f\"{ticker}_Volume\"]].dropna().copy()\n",
    "df_t.set_index(\"Date\", inplace=True)  #  Ensure time-based indexing\n",
    "\n",
    "# Compute daily returns and volume changes\n",
    "df_t[\"Return_1d\"] = df_t[f\"{ticker}_Price\"].pct_change()\n",
    "df_t[\"VolChange\"] = df_t[f\"{ticker}_Volume\"].pct_change()\n",
    "\n",
    "# Compute 5-day returns and rolling volatility\n",
    "df_t[\"Return_5d\"] = df_t[f\"{ticker}_Price\"].pct_change(5)\n",
    "df_t[\"Volatility_5d\"] = df_t[\"Return_1d\"].rolling(5).std()\n",
    "\n",
    "# Drop missing values after feature creation\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# Binary target: 1 = Up day, 0 = Down day\n",
    "df_t[\"Target\"] = (df_t[\"Return_1d\"].shift(-1) > 0).astype(int)\n",
    "\n",
    "# Preview last few rows\n",
    "df_t.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74584e30-9396-4bc4-bf70-34e95fea889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e942972-0558-4193-b7a3-dc39f855fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec7307-7f36-4602-969d-6ebe30c8f554",
   "metadata": {},
   "source": [
    "## Noisy line for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03c763-2805-4e89-bb12-66300692a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use daily returns from your ADR data (already computed)\n",
    "df_t[\"Next_Return\"] = df_t[\"Return_1d\"].shift(-1)  # next-day target\n",
    "df_t.dropna(inplace=True)\n",
    "\n",
    "# Extract variables\n",
    "Xr = df_t[\"Return_1d\"].values.reshape(-1, 1)   # today's return\n",
    "yr = df_t[\"Next_Return\"].values                # next day's return\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(Xr, yr, s=15, alpha=0.6)\n",
    "plt.xlabel(\"Today's Return\")\n",
    "plt.ylabel(\"Next-Day Return\")\n",
    "plt.title(f\"{ticker} — Return-to-Return Relationship\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample size: {len(df_t)} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090a80a-14cd-4f34-abb2-dd50e92dcf82",
   "metadata": {},
   "source": [
    "- The x-axis shows today’s daily return `(Return_1d)`.\n",
    "- The y-axis shows the next day’s return `(Next_Return)`.\n",
    "- Each dot = one trading day.\n",
    "- The *dense circular cloud near (0, 0)* shows that most daily moves are small, clustered around zero.\n",
    "- The slight elliptical tilt (if any) tells us whether positive days tend to be followed by positive days — i.e., **autocorrelation in returns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a89ea-f5a5-4e2d-8c02-f136ee48b63a",
   "metadata": {},
   "source": [
    "## Linear regression with metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626894f6-f514-4c75-88df-26cc7165bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3.2 Linear Regression on Financial Returns\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- Define X (today's return) and y (next day's return)\n",
    "Xr = df_t[\"Return_1d\"].shift(1).dropna().values.reshape(-1, 1)\n",
    "yr = df_t[\"Return_1d\"].iloc[1:].values\n",
    "\n",
    "# --- Fit linear regression model\n",
    "linreg = LinearRegression().fit(Xr, yr)\n",
    "yr_pred = linreg.predict(Xr)\n",
    "\n",
    "# --- Metrics\n",
    "mae = mean_absolute_error(yr, yr_pred)\n",
    "mse = mean_squared_error(yr, yr_pred)\n",
    "r2 = r2_score(yr, yr_pred)\n",
    "\n",
    "print(f\"{ticker} — Linear Regression Results (Return → Next Return)\")\n",
    "print(f\"Coefficient (slope): {linreg.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {linreg.intercept_:.4f}\")\n",
    "print(f\"MAE: {mae:.5f}\")\n",
    "print(f\"MSE: {mse:.5f}\")\n",
    "print(f\"R²: {r2:.5f}\")\n",
    "\n",
    "# --- Visualization\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(Xr, yr, s=15, alpha=0.3, label=\"Observed\")\n",
    "plt.plot(np.sort(Xr, axis=0), linreg.predict(np.sort(Xr, axis=0)), \n",
    "         color=\"red\", lw=2, label=\"Linear Fit\")\n",
    "plt.xlabel(\"Today's Return\")\n",
    "plt.ylabel(\"Next-Day Return\")\n",
    "plt.title(f\"{ticker} — Linear Regression Fit\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261bbb7-30fc-435c-8229-b50b3d241ef0",
   "metadata": {},
   "source": [
    "### **Interpretation: Linear Return-to-Return Fit (CIB)**\n",
    "\n",
    "| Metric             | Value   | Interpretation                                                                                                                           |\n",
    "| :----------------- | :------ | :--------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Slope (β₁)**     | 0.0703  | Slight positive serial correlation — a small tendency for positive days to be followed by positive days, but economically insignificant. |\n",
    "| **Intercept (β₀)** | 0.0005  | Negligible drift — the average next-day return when today’s return is zero.                                                              |\n",
    "| **MAE**            | 0.0158  | Mean absolute forecast error roughly equal to CIB’s daily volatility (~1.6%), confirming high noise.                                     |\n",
    "| **MSE**            | 0.00053 | Reflects variance magnitude of daily returns.                                                                                            |\n",
    "| **R²**             | 0.0049  | Less than 1% of next-day variation explained — statistically meaningless predictive power.                                               |\n",
    "\n",
    "---\n",
    "\n",
    "### **Insight**\n",
    "\n",
    "* The fitted line’s **tiny upward tilt** confirms that linear models capture only trivial autocorrelation.\n",
    "* Financial returns are **near-white-noise**, consistent with market efficiency at daily horizons.\n",
    "* The linear regression essentially models *random scatter around zero* — it fits structure where little exists.\n",
    "* In practice, quants move beyond linear fits toward **nonlinear, regime-aware, or cross-sectional models** to find real signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1aecc-522a-441e-91bc-1cc734318806",
   "metadata": {},
   "source": [
    "## Beyond the Simple Line — Ridge and Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf922fd-9841-4e39-9b54-00c92411136e",
   "metadata": {},
   "source": [
    "### Ridge Regression — Controlling Overfitting\n",
    "**Regularization test with varying α (λ) values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672c8ac-add5-4f73-94de-09c4dcc501ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4.1 Ridge Regression — Controlling Overfitting\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "ridge_results = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a).fit(Xr, yr)\n",
    "    yr_pred = ridge.predict(Xr)\n",
    "    r2 = r2_score(yr, yr_pred)\n",
    "    ridge_results.append((a, ridge.coef_[0], ridge.intercept_, r2))\n",
    "\n",
    "ridge_df = pd.DataFrame(ridge_results, columns=[\"alpha\", \"coef\", \"intercept\", \"R2\"])\n",
    "display(ridge_df.round(5))\n",
    "\n",
    "# --- Visualization\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(ridge_df[\"alpha\"], ridge_df[\"R2\"], marker=\"o\", label=\"R² Score\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Regularization Strength (alpha)\")\n",
    "plt.ylabel(\"R²\")\n",
    "plt.title(f\"{ticker} — Ridge Regression: Regularization Effect\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0400c0-5d76-4b6d-8609-153e895cb356",
   "metadata": {},
   "source": [
    "### Polynomial Regression — Increasing Model Flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ada79-51ef-4593-990a-a390a8cf233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 4.2 Polynomial Regression — Increasing Model Flexibility\n",
    "# ==============================================================\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degrees = [1, 2, 3, 5]\n",
    "scores = []\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for d in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    model.fit(Xr, yr)\n",
    "    yr_pred = model.predict(Xr)\n",
    "    r2 = r2_score(yr, yr_pred)\n",
    "    scores.append(r2)\n",
    "    \n",
    "    # Plot subset of fitted curve for visualization\n",
    "    x_line = np.linspace(Xr.min(), Xr.max(), 200).reshape(-1, 1)\n",
    "    y_line = model.predict(x_line)\n",
    "    plt.plot(x_line, y_line, label=f\"Degree {d}\")\n",
    "\n",
    "plt.scatter(Xr, yr, s=10, alpha=0.3, color=\"gray\", label=\"Observed\")\n",
    "plt.title(f\"{ticker} — Polynomial Fits (Return → Next Return)\")\n",
    "plt.xlabel(\"Today's Return\")\n",
    "plt.ylabel(\"Next-Day Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Summary Table\n",
    "poly_df = pd.DataFrame({\"Degree\": degrees, \"R²\": np.round(scores, 5)})\n",
    "display(poly_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5db6e0-5ae7-4fcb-904c-5a61794e96fe",
   "metadata": {},
   "source": [
    "### **Explanation**\n",
    "\n",
    "- Increasing the polynomial degree adds curvature and flexibility to the model.\n",
    "- **Degree 1** reproduces the linear fit (baseline).\n",
    "- **Higher degrees** may slightly improve in-sample fit (R² ↑) but mostly capture noise — a visual cue of *overfitting*.\n",
    "- In financial modeling, this illustrates why **parsimonious models** are preferred unless backed by strong domain priors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958e6a0-30d5-4da5-a396-bafa475b0885",
   "metadata": {},
   "source": [
    "| Concept                                      | Observation                                                                                          | Quantitative Takeaway                                                                                                                                                                           |\n",
    "| :------------------------------------------- | :--------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Ridge Regression (Bias Control)**          | The slope shrinks from 0.07 → 0.0009 as α increases. R² declines toward zero.                        | Stronger regularization dampens all coefficients, stabilizing the fit at the cost of explanatory power. Useful for high-dimensional factor spaces, but here, no real signal exists to preserve. |\n",
    "| **Polynomial Regression (Variance Control)** | As degree increases (1→5), R² rises modestly from 0.0049 → 0.03, yet fitted curves oscillate wildly. | Added flexibility captures short-term noise, not structure — an illustration of overfitting. In noisy financial returns, this typically leads to poor out-of-sample generalization.             |\n",
    "| **Bias–Variance Trade-off**                  | Ridge adds bias but lowers variance; Polynomial adds variance but lowers bias.                       | Financial data lives in the high-noise regime — favoring parsimonious, regularized models over complex curves.                                                                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf6c6d-58e9-40ca-8d18-1920118c8ed0",
   "metadata": {},
   "source": [
    "- The weak R² and unstable curves confirm that daily return predictability is statistically negligible.\n",
    "- Regularization helps control model instability, but cannot manufacture predictive power where none exists.\n",
    "- This is the same foundational lesson driving cross-validation and model selection in quant workflows — avoid overfitting transient noise.\n",
    "- Ridge regression analogues often appear in risk-premia research (e.g., shrinkage estimators in covariance or factor models), while polynomial expansions resemble nonlinear kernel effects in modern ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd46fb-0fa8-4f93-b60f-ed558c65f401",
   "metadata": {},
   "source": [
    "### Binary Classification on ADR Daily Returns\n",
    "In this section, we replace the synthetic “two moons” example with a real binary classification task:\n",
    "predicting whether the next day’s return will be up (1) or down (0) based on features such as today’s return and volume change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcfee9-c1c2-4649-962e-8c17f182abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5.1 ADR-Based Binary Classification Setup\n",
    "# ==============================================================\n",
    "\n",
    "# Prepare feature matrix and labels (Up/Down days)\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].to_numpy()\n",
    "y = df_t[\"Target\"].to_numpy()\n",
    "\n",
    "# Chronological split for time-series consistency\n",
    "split = int(0.75 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)} | Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc56cea-a1b9-4cb6-b8fc-690c7cf73252",
   "metadata": {},
   "source": [
    "## Logistic regression and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44009dc4-8751-4685-ba3a-4b612062ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5.2 Logistic Regression — Predicting Up vs. Down Days\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Initialize and fit model\n",
    "logit = LogisticRegression(max_iter=1000)\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = logit.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "\n",
    "# --- Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred,\n",
    "    display_labels=[\"Down\", \"Up\"],\n",
    "    cmap=\"Blues\"\n",
    ")\n",
    "plt.title(f\"{ticker} — Logistic Regression Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0434cd-e325-4ffd-b775-ee8805015d2c",
   "metadata": {},
   "source": [
    "## SVM: linear vs RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158afff4-7fcc-4117-8c6d-64673de26165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5.3 SVM — Linear vs. RBF Kernels on ADR Features\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build pipelines\n",
    "logit_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "rbf_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc_logit = accuracy_score(y_test, logit_pipe.predict(X_test))\n",
    "acc_rbf = accuracy_score(y_test, rbf_pipe.predict(X_test))\n",
    "\n",
    "print(f\"Logistic Regression (Linear) Accuracy: {acc_logit:.3f}\")\n",
    "print(f\"SVM (RBF Kernel) Accuracy: {acc_rbf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e58c2-4876-43fc-8665-c32cdb4bd605",
   "metadata": {},
   "source": [
    "#### Explanation — Linear vs. RBF Kernel on ADR Features\n",
    "\n",
    "- **Logistic Regression (Linear)**:\n",
    "Achieves 99.8% accuracy, showing that daily return and volume change separate up/down days almost linearly.\n",
    "\n",
    "- **SVM (RBF)**:\n",
    "Slightly lower accuracy (99.2%) — added non-linearity captures little extra signal and may overfit short-term noise.\n",
    "\n",
    "+ **Takeaway**:\n",
    "For short-horizon financial data, linear models often suffice; non-linear kernels help only when richer or regime-dependent features are introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14052665-388b-4662-9179-d43b500ac78a",
   "metadata": {},
   "source": [
    "## Decision tree: depth control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f3417-4be8-456a-b530-79dbbc42ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5.4 Decision Tree — Depth Control\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Fit a decision tree with limited depth\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Inspect tree structure and performance\n",
    "depth = tree.get_depth()\n",
    "leaves = tree.get_n_leaves()\n",
    "train_acc = round(tree.score(X_train, y_train), 3)\n",
    "test_acc = round(tree.score(X_test, y_test), 3)\n",
    "\n",
    "print(f\"Depth: {depth}, Leaves: {leaves}, Train Acc: {train_acc}, Test Acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188c391-43af-4906-820c-33ff8b588489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree as sktree\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sktree.plot_tree(\n",
    "    tree,\n",
    "    feature_names=[\"Return_1d\", \"VolChange\"],\n",
    "    class_names=[\"Down\", \"Up\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8\n",
    ")\n",
    "plt.title(f\"{ticker} — Decision Tree (Depth={depth})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a3a29-3523-4369-ae30-a47dfd699802",
   "metadata": {},
   "source": [
    "## Decision Tree Depth Sweep — Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4396b1-b8d2-4a9d-9e9c-35673d1c3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5.5 Add Controlled Noise to Targets (for Realistic Classification)\n",
    "# ==============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "noise_level = 0.05  # 5% label noise\n",
    "\n",
    "df_t[\"Target_noisy\"] = df_t[\"Target\"].copy()\n",
    "flip_mask = np.random.rand(len(df_t)) < noise_level\n",
    "df_t.loc[flip_mask, \"Target_noisy\"] = 1 - df_t.loc[flip_mask, \"Target_noisy\"]\n",
    "\n",
    "# Use noisy labels for training\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target_noisy\"].values\n",
    "\n",
    "split = int(0.75 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd626274-9e27-4d37-80e2-ddd7aa19fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 2, 3, 4, 6, 8, 10]\n",
    "train_scores, test_scores = [], []\n",
    "\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_scores.append(clf.score(X_train, y_train))\n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(depths, train_scores, \"o--\", label=\"Train Accuracy\")\n",
    "plt.plot(depths, test_scores, \"s-\", label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Tree Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"{ticker} — Decision Tree Depth Sweep (with 5% label noise)\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bbd1e-4b38-453f-87bb-ade3cc56bc58",
   "metadata": {},
   "source": [
    "## Ensemble Methods — Random Forest Classifier\n",
    "Random Forests combine multiple decision trees trained on different data subsamples to reduce variance while preserving predictive power.\n",
    "They’re ideal for noisy or unstable data — just like financial time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847c45f-3a98-4a64-bbd5-9fc0d592ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 6. Random Forest Classifier — Ensemble of Decision Trees\n",
    "# ==============================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use the same features and 5% noisy labels\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target_noisy\"].values\n",
    "\n",
    "# Chronological split\n",
    "split = int(0.75 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# --- Train Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,       # number of trees\n",
    "    max_depth=5,            # limit depth for interpretability\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, rf.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Random Forest — Train Accuracy: {acc_train:.3f} | Test Accuracy: {acc_test:.3f}\")\n",
    "\n",
    "# --- Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=[\"Down\", \"Up\"])\n",
    "plt.title(f\"{ticker} — Random Forest Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b52550-1b8c-4633-9468-852309ad7ec8",
   "metadata": {},
   "source": [
    "## Visualizing Ensemble Size Effect (Bias–Variance Tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf353dde-6a56-455f-acf6-60ab4c79c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensemble Size Sweep\n",
    "n_trees = [1, 5, 10, 25, 50, 100, 200]\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "for n in n_trees:\n",
    "    rf = RandomForestClassifier(n_estimators=n, max_depth=5, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_acc.append(accuracy_score(y_train, rf.predict(X_train)))\n",
    "    test_acc.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(n_trees, train_acc, \"o--\", label=\"Train Accuracy\")\n",
    "plt.plot(n_trees, test_acc, \"s-\", label=\"Test Accuracy\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Trees (log scale)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"{ticker} — Random Forest Ensemble Size Effect\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb54a4e-bd1c-40b8-a223-d7516a82f848",
   "metadata": {},
   "source": [
    "## Feature Importance in Random Forests (CIB ADR Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6adcd-78c4-43bb-8928-dcaea883206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 7. Feature Importance — Understanding Model Drivers\n",
    "# ==============================================================\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute feature importances\n",
    "importances = rf.feature_importances_\n",
    "features = [\"Return_1d\", \"VolChange\"]\n",
    "\n",
    "# Wrap in DataFrame for clarity\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        \"Feature\": features,\n",
    "        \"Importance\": importances\n",
    "    })\n",
    "    .sort_values(\"Importance\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- Plot (future-proofed)\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.barplot(\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    hue=\"Feature\",          # required in seaborn ≥ 0.14\n",
    "    data=imp_df,\n",
    "    palette=\"viridis\",\n",
    "    legend=False            # suppress redundant legend\n",
    ")\n",
    "plt.title(f\"{ticker} — Random Forest Feature Importance\")\n",
    "plt.xlabel(\"Importance (relative)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Optional: add numeric labels\n",
    "for i, (imp, feat) in enumerate(zip(imp_df[\"Importance\"], imp_df[\"Feature\"])):\n",
    "    plt.text(\n",
    "        x=imp + 0.005,       # position slightly right of bar\n",
    "        y=i, \n",
    "        s=f\"{imp:.3f}\",\n",
    "        va=\"center\"\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Display numeric values\n",
    "display(imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a57e9-9095-4d25-8d3d-5130775f1689",
   "metadata": {},
   "source": [
    "## Logistic Probability Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317ae3b-285e-4316-8a2c-de4615fe86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Logistic Probability Contour (Optional)\n",
    "# ==============================================================\n",
    "\n",
    "# --- Fit logistic model on standardized features\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ").fit(X, y)   # use your final X and y (already defined above)\n",
    "\n",
    "# --- Define grid limits based on your data\n",
    "xmin, xmax = X[:, 0].min() - 0.6, X[:, 0].max() + 0.6\n",
    "ymin, ymax = X[:, 1].min() - 0.6, X[:, 1].max() + 0.6   # moved to its own line\n",
    "\n",
    "# --- Mesh grid for probability contour\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xmin, xmax, 200),\n",
    "    np.linspace(ymin, ymax, 200)\n",
    ")\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "proba = pipe.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# --- Plot probability surface\n",
    "plt.figure(figsize=(5, 4))\n",
    "cs = plt.contourf(xx, yy, proba, levels=21, cmap='coolwarm', alpha=0.8)\n",
    "plt.contour(xx, yy, proba, levels=[0.5], colors='k', linewidths=1.0)\n",
    "\n",
    "# Overlay your actual training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k', s=25)\n",
    "\n",
    "plt.colorbar(cs, label='P(class = 1)')\n",
    "plt.xlabel(\"Return_1d\")\n",
    "plt.ylabel(\"VolChange\")\n",
    "plt.title(f\"{ticker} — Logistic Probability Contour\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a997e8-ad90-46d3-9eec-fae84c8ff2b5",
   "metadata": {},
   "source": [
    "## Residual Diagnostics Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3dc1b5-243b-4f02-ba0c-f0104c77d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3.3 Residual Analysis — Checking Model Fit\n",
    "# ==============================================================\n",
    "\n",
    "# Residuals\n",
    "residuals = yr - yr_pred\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(Xr, residuals, s=15, alpha=0.5)\n",
    "plt.axhline(0, color=\"black\", lw=1, linestyle=\"--\")\n",
    "plt.title(f\"{ticker} — Linear Regression Residuals\")\n",
    "plt.xlabel(\"Today's Return\")\n",
    "plt.ylabel(\"Residual (Observed - Predicted)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Residual Distribution\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(residuals, bins=40, color=\"gray\", alpha=0.7, edgecolor=\"black\")\n",
    "plt.title(f\"{ticker} — Residual Distribution\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Optional: Autocorrelation of residuals\n",
    "import statsmodels.api as sm\n",
    "sm.graphics.tsa.plot_acf(residuals, lags=20)\n",
    "plt.title(f\"{ticker} — Residual Autocorrelation (Lags=20)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2799eb-09d5-48fa-94ce-7dd02f62b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 8. Model Comparison Summary — Classical ML on ADR Features\n",
    "# ==============================================================\n",
    "\n",
    "# Use clean binary target\n",
    "X = df_t[[\"Return_1d\", \"VolChange\"]].values\n",
    "y = df_t[\"Target\"].values\n",
    "\n",
    "# Chronological split\n",
    "split = int(0.75 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# --- Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000)),\n",
    "    \"SVM (RBF Kernel)\": make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# --- Fit and evaluate\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    gap = acc_train - acc_test\n",
    "    results.append({\"Model\": name, \"Train Acc\": acc_train, \"Test Acc\": acc_test, \"Overfit Gap\": gap})\n",
    "\n",
    "# --- Summary table\n",
    "summary_df = pd.DataFrame(results).sort_values(\"Test Acc\", ascending=False).reset_index(drop=True)\n",
    "summary_df.style.format({\n",
    "    \"Train Acc\": \"{:.3f}\", \"Test Acc\": \"{:.3f}\", \"Overfit Gap\": \"{:.3f}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05c866-4505-4eb3-bb85-6a9b163395ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(summary_df[\"Model\"], summary_df[\"Test Acc\"], color=\"steelblue\")\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(f\"{ticker} — Model Comparison Summary\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
